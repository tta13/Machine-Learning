{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOexrx9AWJT/Pxy69bKCvkE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "Ep2PCJkdjCLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different encoding techniques"
      ],
      "metadata": {
        "id": "_r7bCJTgjFQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of words\n",
        "* Transforms words into numbers using a dictionay.\n",
        "* We only care about the frequency of words in a sentence.\n",
        "* Bad because it doesn't take into account the ordering of those words!"
      ],
      "metadata": {
        "id": "w_sq6p3skOXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-EkYHg6gyYX",
        "outputId": "b2662967-2a78-484e-9a4a-ffcc11307dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ],
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding\n",
        "      word_encoding += 1\n",
        "    \n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "  \n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word embeddings\n",
        "* Tranforms words into a N dimensional vector.\n",
        "* Tries to make similar words point to similar directions and antonym words in opposite directions.\n",
        "* It's a layer of the model."
      ],
      "metadata": {
        "id": "a1JqwVjFlt2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setiment Analysis with Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "_gCZ7KA6nN_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 400\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNj26r1UpmRn",
        "outputId": "8f61bbe1-6aa8-46ad-ad1b-e8be53e3814c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyo2qyQJsi69",
        "outputId": "c09cac4c-d2df-4092-f439-21d90fbf1867"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r06Fh2z7p42f",
        "outputId": "73afff70-dbfb-451c-971f-7c2aa75e29e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "550"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding reviews so that they can be the same size."
      ],
      "metadata": {
        "id": "nuFeb3lNqH9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "metadata": {
        "id": "ZHJZzM0DqMAM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJn5mMnurHiQ",
        "outputId": "1ea816ed-bc40-40d4-fe35-c655819dc6fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the model"
      ],
      "metadata": {
        "id": "WgcRMKBArFUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 16),\n",
        "    tf.keras.layers.LSTM(16),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "9pe2gDGdrEYZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwFMKg6yrdwy",
        "outputId": "99119e52-0a83-4327-eb23-9d1a642c949b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          1417344   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 16)                2112      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,419,473\n",
            "Trainable params: 1,419,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "T7LjCH-grvOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "IFWckgu1rwfI",
        "outputId": "c8df182d-7a32-4f19-814d-ad422645a3f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 57s 81ms/step - loss: 0.4683 - acc: 0.7787 - val_loss: 0.3286 - val_acc: 0.8668\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 51s 82ms/step - loss: 0.2501 - acc: 0.9032 - val_loss: 0.2867 - val_acc: 0.8836\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 49s 79ms/step - loss: 0.1848 - acc: 0.9318 - val_loss: 0.2722 - val_acc: 0.8934\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 49s 79ms/step - loss: 0.1531 - acc: 0.9466 - val_loss: 0.3027 - val_acc: 0.8964\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 49s 79ms/step - loss: 0.1272 - acc: 0.9553 - val_loss: 0.3246 - val_acc: 0.8946\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 51s 82ms/step - loss: 0.1102 - acc: 0.9624 - val_loss: 0.2768 - val_acc: 0.8928\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 49s 79ms/step - loss: 0.1001 - acc: 0.9653 - val_loss: 0.4219 - val_acc: 0.8824\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 50s 79ms/step - loss: 0.0927 - acc: 0.9695 - val_loss: 0.3220 - val_acc: 0.8792\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 51s 82ms/step - loss: 0.0805 - acc: 0.9730 - val_loss: 0.3946 - val_acc: 0.8878\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 50s 79ms/step - loss: 0.0742 - acc: 0.9758 - val_loss: 0.4209 - val_acc: 0.8668\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-522b4a4a4ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rmsprop\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dof9FrJavBkE",
        "outputId": "123cd672-eaa9-4cb2-d842-52cfd9653bc4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 18s 23ms/step - loss: 0.4695 - acc: 0.8480\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4694586992263794, 0.848039984703064]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting"
      ],
      "metadata": {
        "id": "1EVEwH48vCjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kDS9082vDme",
        "outputId": "419d70f5-2379-42a9-9ef1-89aad2619192"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  12  17  13\n",
            "  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# while were at it lets make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItOaMcH8vFR4",
        "outputId": "0efdd604-fef3-44ce-ce4f-d8f4041ba5b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,MAXLEN))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was fire, really loved it and would watch it again because it was great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"That movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)\n",
        "\n",
        "review = \"really nice movie nice nice nice nice nice nice nice nice nice nice\"\n",
        "predict(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_EDmLApvHhB",
        "outputId": "8f084e94-f9ff-4e53-9cc0-246f82ce72e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9423091]\n",
            "[0.44977587]\n",
            "[0.780455]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play Generator with RNN"
      ],
      "metadata": {
        "id": "_H4k8dKDKHHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OJdQ2ANKK4m",
        "outputId": "1aa15081-a94a-48a6-e959-687dba68ef40"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "YmlB2rVAKQOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zlix7GN-KPQq",
        "outputId": "9094e6ee-21ac-4947-8dcf-5d2c79804b4e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading file content"
      ],
      "metadata": {
        "id": "MIq8oiXTK1ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "'Length of text: {} characters'.format(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gLyguqdkK3X3",
        "outputId": "5cf360f0-be7c-4bd2-89c0-a4a365616f69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Length of text: 1115394 characters'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:280])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMxFcx-bLAtd",
        "outputId": "a827e419-5696-4dc4-cbd1-0171d4020db3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding text"
      ],
      "metadata": {
        "id": "qzXGDPrDLb1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "2zjLvAt6LdRG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "max_index = 13\n",
        "print(\"Text:\", text[:max_index])\n",
        "print(\"Encoded:\", text_to_int(text[:max_index]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KptU-XPkL8SK",
        "outputId": "a6c5e356-d39c-4b7c-bf9f-002748001ad5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:max_index]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BqH1XHKMMo5",
        "outputId": "76e03496-720c-479a-ab48-5f5540715f54"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating training examples"
      ],
      "metadata": {
        "id": "Puasb5PNMXw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "fMyiqx19MZ1H"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "7i01X-x_M7Ck"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "J4LMqArHNCDb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A2FAZKLNKGj",
        "outputId": "67ea1099-ed14-4589-bd3d-16957ecddc66"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "0oXQhEXENUsR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ],
      "metadata": {
        "id": "aoXlmbv2Njs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxdl_w0yNi7N",
        "outputId": "47b78e98-ce7b-4db1-b7c5-679f0a8e6e88"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the loss function"
      ],
      "metadata": {
        "id": "oogvF54_Q4Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNyq4IhDQ6I0",
        "outputId": "1defb7b3-7cb1-4ab5-c72f-e88f8f1523b7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "id": "1SPapWqARfnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR5m6lQERwiK",
        "outputId": "ae2fec09-9d95-4bae-a97d-c3a29a7e9eae"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00123202  0.00097746  0.00101778 ...  0.00276049  0.00184684\n",
            "  -0.00332387]\n",
            " [-0.00216997  0.0024099   0.00220178 ...  0.00435392  0.00351144\n",
            "  -0.00552667]\n",
            " [-0.00233137  0.00326997  0.00224819 ... -0.00184656  0.00074331\n",
            "  -0.00148076]\n",
            " ...\n",
            " [-0.00563261  0.0114264   0.01418458 ... -0.00678292 -0.0086744\n",
            "  -0.00274013]\n",
            " [-0.00923069  0.0065335   0.01315108 ... -0.00440346 -0.00626947\n",
            "   0.00104885]\n",
            " [-0.00992797  0.00795599  0.00559041 ... -0.00838242 -0.00490273\n",
            "   0.000685  ]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHU1WO7JR_5d",
        "outputId": "8fcfbbb4-aa32-40f3-cd00-9b59a0324f0b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-0.00123202  0.00097746  0.00101778  0.00104992  0.00510853  0.00259233\n",
            "  0.00207283 -0.00449835  0.0026038   0.00436174  0.0001319  -0.00139873\n",
            " -0.00450894 -0.00300498 -0.00336798 -0.0045438  -0.00343565 -0.00141875\n",
            "  0.00138818  0.0010282   0.00538143  0.00257647 -0.00293807  0.000276\n",
            "  0.00023713 -0.00764048 -0.00262197 -0.00073794 -0.00056239  0.00157786\n",
            " -0.00347422  0.00151969 -0.00332236  0.00057631 -0.00231169 -0.00410942\n",
            "  0.00045926 -0.00443494 -0.00159698  0.00152113 -0.00078343  0.00523129\n",
            "  0.00467792 -0.00203488  0.00111008 -0.00595684  0.00188871 -0.00039446\n",
            " -0.002462   -0.00461904 -0.00212037 -0.00335748 -0.00185713 -0.00033557\n",
            "  0.00248588  0.00243644  0.0037819  -0.00026572  0.00125178 -0.00012109\n",
            "  0.00228718  0.00510106  0.00276049  0.00184684 -0.00332387], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bMgG48cbSNJA",
        "outputId": "f19b9a11-4b87-40ed-e74a-e14c11f04081"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'tS?ceA$uJZPesvoEMdWAZ'EVFXvH,qzj;kZ\\nHonF!AdGMLXydOIQc, zX-j\\nqK \\nJM?SwF!\\nUCQ'i;-Pr-t:a;qQBnEKHgKHNRi\""
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "u2Zv2Si2Styw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling the model"
      ],
      "metadata": {
        "id": "mA0CI-LrS-02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0AxBWscTBJi",
        "outputId": "c144180c-4734-4829-aa63-13297ab4e2b3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up checkpoints"
      ],
      "metadata": {
        "id": "REHCVIpiTW8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "6eMojq_5TJ4a"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "bPhOcDmFTk8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "0hrtzJ55TmWd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcyxNvUMTpAN",
        "outputId": "c9f16d4a-5f60-4725-dfbb-dcbbf3245939"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 31s 159ms/step - loss: 2.6054\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 1.8979\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.6484\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 1.5122\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 30s 165ms/step - loss: 1.4299\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 30s 165ms/step - loss: 1.3731\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 30s 165ms/step - loss: 1.3281\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 30s 165ms/step - loss: 1.2893\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 1.2529\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 1.2182\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 1.1827\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 1.1477\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 1.1106\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 1.0712\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 1.0316\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 0.9906\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 0.9496\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 0.9084\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 0.8676\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 30s 164ms/step - loss: 0.8303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions\n",
        "We'll need to load the model from a checkpoint using a batch size of 1 so we can feed text and make the model predict the next characters."
      ],
      "metadata": {
        "id": "eQQUKBocXb_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "JIWzzniyXt5C"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "zkHvKQ9vXyX5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(f\"./training_checkpoints/ckpt_{checkpoint_num}\"))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "ItJm2q9mXz9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating text from input"
      ],
      "metadata": {
        "id": "VsBHGwPhYPmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_generate=800):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "ETdlSLWNYOQp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_string = input(\"Type a starting string: \")\n",
        "length = int(input(\"Number of characters to generate: \"))\n",
        "print(generate_text(model, start_string, length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NxNtcxMYh92",
        "outputId": "5aaa7074-f119-445e-f405-567948362f9b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: Yes sir\n",
            "Number of characters to generate: 20\n",
            "Yes sir,\n",
            "Which, brought him\n"
          ]
        }
      ]
    }
  ]
}